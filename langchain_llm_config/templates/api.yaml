llm:
  # OpenAI config
  openai:
    chat:
      api_base: "https://api.siliconflow.cn" # leave empty for default openai base url
      api_key: "${OPENAI_API_KEY}" # Have to be exported in the environment, or write in your .env file
      model_name: "Qwen/Qwen2.5-7B-Instruct" 
      temperature: 0.7
      max_tokens: 8192
      connect_timeout: 30
      read_timeout: 60
    embeddings:
      api_base: "https://api.siliconflow.cn" # leave empty for default openai base url
      api_key: "${OPENAI_API_KEY}" # Have to be exported in the environment, or write in your .env file
      model_name: "BAAI/bge-m3"
      dimensions: 1024
      timeout: 30

  # VLLM config
  vllm:
    chat:
      api_base: "https://api.siliconflow.cn"
      api_key: "${OPENAI_API_KEY}" # Have to be exported in the environment, or write in your .env file
      model_name: "Qwen/Qwen2.5-7B-Instruct"
      temperature: 0.6
      top_p: 0.8
      max_tokens: 8192
      connect_timeout: 30
      read_timeout: 60
    embeddings:
      api_base: "https://api.siliconflow.cn"
      api_key: "${OPENAI_API_KEY}" # Have to be exported in the environment, or write in your .env file
      model_name: "BAAI/bge-m3"
      dimensions: 1024
      timeout: 30

  # Gemini config
  gemini:
    chat:
      api_key: "${GEMINI_API_KEY}" # Have to be exported in the environment, or write in your .env file
      model_name: "gemini-pro"
      temperature: 0.7
      max_tokens: 8192
      top_p: 1.0
      connect_timeout: 30
      read_timeout: 60
    embeddings:
      api_key: "${GEMINI_API_KEY}" # Have to be exported in the environment, or write in your .env file
      model_name: "text-embedding-004"  # Google's text embedding model
      timeout: 30

  # Infinity config
  infinity:
    embeddings:
      api_base: "http://localhost:7997/v1"
      model_name: "models/bge-m3" # Fill in the model name

  # Default provider to use
  default:
    chat_provider: "vllm"  # Options: openai, vllm, gemini
    embedding_provider: "vllm"  # Options: openai, infinity, gemini